{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import setup_path \n",
    "import airsim\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "from cntk.core import Value\n",
    "from cntk.initializer import he_uniform\n",
    "from cntk.layers import Sequential, Convolution2D, Dense, default_options\n",
    "from cntk.layers.typing import Signature, Tensor\n",
    "from cntk.learners import adam, learning_rate_schedule, momentum_schedule, UnitType\n",
    "from cntk.logging import TensorBoardProgressWriter\n",
    "from cntk.ops import abs, argmax, element_select, less, relu, reduce_max, reduce_sum, square\n",
    "from cntk.ops.functions import CloneMethod, Function\n",
    "from cntk.train import Trainer\n",
    "\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def getScreenDepthVis():\n",
    "\n",
    "    responses = client.simGetImages([airsim.ImageRequest(0, airsim.ImageType.DepthPerspective, True, False)])\n",
    "    img1d = np.array(responses[0].image_data_float, dtype=np.float)\n",
    "    img1d = 255/np.maximum(np.ones(img1d.size), img1d)\n",
    "    img2d = np.reshape(img1d, (responses[0].height, responses[0].width))\n",
    "\n",
    "    image = np.invert(np.array(Image.fromarray(img2d.astype(np.uint8), mode='L')))\n",
    "\n",
    "    factor = 10\n",
    "    maxIntensity = 255.0 # depends on dtype of image data\n",
    "\n",
    "    # Decrease intensity such that dark pixels become much darker, bright pixels become slightly dark \n",
    "    newImage1 = (maxIntensity)*(image/maxIntensity)**factor\n",
    "    newImage1 = np.array(newImage1,dtype=np.uint8)\n",
    "\n",
    "    #cv2.imshow(\"Test\", newImage1)\n",
    "    #cv2.waitKey(0)\n",
    "\n",
    "    return newImage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"\n",
    "    ReplayMemory keeps track of the environment dynamic.\n",
    "    We store all the transitions (s(t), action, s(t+1), reward, done).\n",
    "    The replay memory allows us to efficiently sample mini-batches from it, and generate the correct state representation\n",
    "    (w.r.t the number of previous frames needed).\n",
    "    \"\"\"\n",
    "    def __init__(self, size, sample_shape, history_length=4):\n",
    "        self._pos = 0\n",
    "        self._count = 0\n",
    "        self._max_size = size\n",
    "        self._history_length = max(1, history_length)\n",
    "        self._state_shape = sample_shape\n",
    "        self._states = np.zeros((size,) + sample_shape, dtype=np.float32)\n",
    "        self._actions = np.zeros(size, dtype=np.uint8)\n",
    "        self._rewards = np.zeros(size, dtype=np.float32)\n",
    "        self._terminals = np.zeros(size, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Returns the number of items currently present in the memory\n",
    "        Returns: Int >= 0\n",
    "        \"\"\"\n",
    "        return self._count\n",
    "\n",
    "    def append(self, state, action, reward, done):\n",
    "        \"\"\" Appends the specified transition to the memory.\n",
    "\n",
    "        Attributes:\n",
    "            state (Tensor[sample_shape]): The state to append\n",
    "            action (int): An integer representing the action done\n",
    "            reward (float): An integer representing the reward received for doing this action\n",
    "            done (bool): A boolean specifying if this state is a terminal (episode has finished)\n",
    "        \"\"\"\n",
    "        assert state.shape == self._state_shape, \\\n",
    "            'Invalid state shape (required: %s, got: %s)' % (self._state_shape, state.shape)\n",
    "\n",
    "        self._states[self._pos] = state\n",
    "        self._actions[self._pos] = action\n",
    "        self._rewards[self._pos] = reward\n",
    "        self._terminals[self._pos] = done\n",
    "\n",
    "        self._count = max(self._count, self._pos + 1)\n",
    "        self._pos = (self._pos + 1) % self._max_size\n",
    "\n",
    "    def sample(self, size):\n",
    "        \"\"\" Generate size random integers mapping indices in the memory.\n",
    "            The returned indices can be retrieved using #get_state().\n",
    "            See the method #mini-batch() if you want to retrieve samples directly.\n",
    "\n",
    "        Attributes:\n",
    "            size (int): The mini-batch size\n",
    "\n",
    "        Returns:\n",
    "             Indexes of the sampled states ([int])\n",
    "        \"\"\"\n",
    "\n",
    "        # Local variable access is faster in loops\n",
    "        count, pos, history_len, terminals = self._count - 1, self._pos, \\\n",
    "                                             self._history_length, self._terminals\n",
    "        indexes = []\n",
    "\n",
    "        while len(indexes) < size:\n",
    "            index = np.random.randint(history_len, count)\n",
    "\n",
    "            if index not in indexes:\n",
    "\n",
    "                # if not wrapping over current pointer,\n",
    "                # then check if there is terminal state wrapped inside\n",
    "                if not (index >= pos > index - history_len):\n",
    "                    if not terminals[(index - history_len):index].any():\n",
    "                        indexes.append(index)\n",
    "\n",
    "        return indexes\n",
    "\n",
    "    def minibatch(self, size):\n",
    "        \"\"\" Generate a minibatch with the number of samples specified by the size parameter.\n",
    "\n",
    "        Attributes:\n",
    "            size (int): Minibatch size\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tensor[minibatch_size, input_shape...], [int], [float], [bool]\n",
    "        \"\"\"\n",
    "        indexes = self.sample(size)\n",
    "\n",
    "        pre_states = np.array([self.get_state(index) for index in indexes], dtype=np.float32)\n",
    "        post_states = np.array([self.get_state(index + 1) for index in indexes], dtype=np.float32)\n",
    "        actions = self._actions[indexes]\n",
    "        rewards = self._rewards[indexes]\n",
    "        dones = self._terminals[indexes]\n",
    "\n",
    "        return pre_states, actions, post_states, rewards, dones\n",
    "\n",
    "    def get_state(self, index):\n",
    "        \"\"\"\n",
    "        Return the specified state with the replay memory. A state consists of\n",
    "        the last `history_length` perceptions.\n",
    "\n",
    "        Attributes:\n",
    "            index (int): State's index\n",
    "\n",
    "        Returns:\n",
    "            State at specified index (Tensor[history_length, input_shape...])\n",
    "        \"\"\"\n",
    "        if self._count == 0:\n",
    "            raise IndexError('Empty Memory')\n",
    "\n",
    "        index %= self._count\n",
    "        history_length = self._history_length\n",
    "\n",
    "        # If index > history_length, take from a slice\n",
    "        if index >= history_length:\n",
    "            return self._states[(index - (history_length - 1)):index + 1, ...]\n",
    "        else:\n",
    "            indexes = np.arange(index - history_length + 1, index + 1)\n",
    "            return self._states.take(indexes, mode='wrap', axis=0)\n",
    "\n",
    "class History(object):\n",
    "    \"\"\"\n",
    "    Accumulator keeping track of the N previous frames to be used by the agent\n",
    "    for evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, shape):\n",
    "        self._buffer = np.zeros(shape, dtype=np.float32)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        \"\"\" Underlying buffer with N previous states stacked along first axis\n",
    "\n",
    "        Returns:\n",
    "            Tensor[shape]\n",
    "        \"\"\"\n",
    "        return self._buffer\n",
    "\n",
    "    def append(self, state):\n",
    "        \"\"\" Append state to the history\n",
    "\n",
    "        Attributes:\n",
    "            state (Tensor) : The state to append to the memory\n",
    "        \"\"\"\n",
    "        self._buffer[:-1] = self._buffer[1:]\n",
    "        self._buffer[-1] = state\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Reset the memory. Underlying buffer set all indexes to 0\n",
    "\n",
    "        \"\"\"\n",
    "        self._buffer.fill(0)\n",
    "\n",
    "class LinearEpsilonAnnealingExplorer(object):\n",
    "    \"\"\"\n",
    "    Exploration policy using Linear Epsilon Greedy\n",
    "\n",
    "    Attributes:\n",
    "        start (float): start value\n",
    "        end (float): end value\n",
    "        steps (int): number of steps between start and end\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, start, end, steps):\n",
    "        self._start = start\n",
    "        self._stop = end\n",
    "        self._steps = steps\n",
    "\n",
    "        self._step_size = (end - start) / steps\n",
    "\n",
    "    def __call__(self, num_actions):\n",
    "        \"\"\"\n",
    "        Select a random action out of `num_actions` possibilities.\n",
    "\n",
    "        Attributes:\n",
    "            num_actions (int): Number of actions available\n",
    "        \"\"\"\n",
    "        return np.random.choice(num_actions)\n",
    "\n",
    "    def _epsilon(self, step):\n",
    "        \"\"\" Compute the epsilon parameter according to the specified step\n",
    "\n",
    "        Attributes:\n",
    "            step (int)\n",
    "        \"\"\"\n",
    "        if step < 0:\n",
    "            return self._start\n",
    "        elif step > self._steps:\n",
    "            return self._stop\n",
    "        else:\n",
    "            return self._step_size * step + self._start\n",
    "\n",
    "    def is_exploring(self, step):\n",
    "        \"\"\" Commodity method indicating if the agent should explore\n",
    "\n",
    "        Attributes:\n",
    "            step (int) : Current step\n",
    "\n",
    "        Returns:\n",
    "             bool : True if exploring, False otherwise\n",
    "        \"\"\"\n",
    "        return np.random.rand() < self._epsilon(step)\n",
    "\n",
    "def huber_loss(y, y_hat, delta):\n",
    "    \"\"\" Compute the Huber Loss as part of the model graph\n",
    "\n",
    "    Huber Loss is more robust to outliers. It is defined as:\n",
    "     if |y - y_hat| < delta :\n",
    "        0.5 * (y - y_hat)**2\n",
    "    else :\n",
    "        delta * |y - y_hat| - 0.5 * delta**2\n",
    "\n",
    "    Attributes:\n",
    "        y (Tensor[-1, 1]): Target value\n",
    "        y_hat(Tensor[-1, 1]): Estimated value\n",
    "        delta (float): Outliers threshold\n",
    "\n",
    "    Returns:\n",
    "        CNTK Graph Node\n",
    "    \"\"\"\n",
    "    half_delta_squared = 0.5 * delta * delta\n",
    "    error = y - y_hat\n",
    "    abs_error = abs(error)\n",
    "\n",
    "    less_than = 0.5 * square(error)\n",
    "    more_than = (delta * abs_error) - half_delta_squared\n",
    "    loss_per_sample = element_select(less(abs_error, delta), less_than, more_than)\n",
    "\n",
    "    return reduce_sum(loss_per_sample, name='loss')\n",
    "\n",
    "class DeepQAgent(object):\n",
    "    \"\"\"\n",
    "    Implementation of Deep Q Neural Network agent like in:\n",
    "        Nature 518. \"Human-level control through deep reinforcement learning\" (Mnih & al. 2015)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, nb_actions,\n",
    "                 gamma=0.99, explorer=LinearEpsilonAnnealingExplorer(1, 0.1, 1000000),\n",
    "                 learning_rate=0.00025, momentum=0.95, minibatch_size=32,\n",
    "                 memory_size=500000, train_after=10000, train_interval=4, target_update_interval=10000,\n",
    "                 monitor=True):\n",
    "        self.input_shape = input_shape\n",
    "        self.nb_actions = nb_actions\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self._train_after = train_after\n",
    "        self._train_interval = train_interval\n",
    "        self._target_update_interval = target_update_interval\n",
    "\n",
    "        self._explorer = explorer\n",
    "        self._minibatch_size = minibatch_size\n",
    "        self._history = History(input_shape)\n",
    "        self._memory = ReplayMemory(memory_size, input_shape[1:], 4)\n",
    "        self._num_actions_taken = 0\n",
    "\n",
    "        # Metrics accumulator\n",
    "        self._episode_rewards, self._episode_q_means, self._episode_q_stddev = [], [], []\n",
    "\n",
    "        # Action Value model (used by agent to interact with the environment)\n",
    "        with default_options(activation=relu, init=he_uniform()):\n",
    "            self._action_value_net = Sequential([\n",
    "                Convolution2D((8, 8), 16, strides=4),\n",
    "                Convolution2D((4, 4), 32, strides=2),\n",
    "                Convolution2D((3, 3), 32, strides=1),\n",
    "                Dense(256, init=he_uniform(scale=0.01)),\n",
    "                Dense(nb_actions, activation=None, init=he_uniform(scale=0.01))\n",
    "            ])\n",
    "        self._action_value_net.update_signature(Tensor[input_shape])\n",
    "\n",
    "        # Target model used to compute the target Q-values in training, updated\n",
    "        # less frequently for increased stability.\n",
    "        self._target_net = self._action_value_net.clone(CloneMethod.freeze)\n",
    "\n",
    "        # Function computing Q-values targets as part of the computation graph\n",
    "        @Function\n",
    "        @Signature(post_states=Tensor[input_shape], rewards=Tensor[()], terminals=Tensor[()])\n",
    "        def compute_q_targets(post_states, rewards, terminals):\n",
    "            return element_select(\n",
    "                terminals,\n",
    "                rewards,\n",
    "                gamma * reduce_max(self._target_net(post_states), axis=0) + rewards,\n",
    "            )\n",
    "\n",
    "        # Define the loss, using Huber Loss (more robust to outliers)\n",
    "        @Function\n",
    "        @Signature(pre_states=Tensor[input_shape], actions=Tensor[nb_actions],\n",
    "                   post_states=Tensor[input_shape], rewards=Tensor[()], terminals=Tensor[()])\n",
    "        def criterion(pre_states, actions, post_states, rewards, terminals):\n",
    "            # Compute the q_targets\n",
    "            q_targets = compute_q_targets(post_states, rewards, terminals)\n",
    "\n",
    "            # actions is a 1-hot encoding of the action done by the agent\n",
    "            q_acted = reduce_sum(self._action_value_net(pre_states) * actions, axis=0)\n",
    "\n",
    "            # Define training criterion as the Huber Loss function\n",
    "            return huber_loss(q_targets, q_acted, 1.0)\n",
    "\n",
    "        # Adam based SGD\n",
    "        lr_schedule = learning_rate_schedule(learning_rate, UnitType.minibatch)\n",
    "        m_schedule = momentum_schedule(momentum)\n",
    "        vm_schedule = momentum_schedule(0.999)\n",
    "        l_sgd = adam(self._action_value_net.parameters, lr_schedule,\n",
    "                     momentum=m_schedule, variance_momentum=vm_schedule)\n",
    "\n",
    "        self._metrics_writer = TensorBoardProgressWriter(freq=1, log_dir='metrics', model=criterion) if monitor else None\n",
    "        self._learner = l_sgd\n",
    "        self._trainer = Trainer(criterion, (criterion, None), l_sgd, self._metrics_writer)\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\" This allows the agent to select the next action to perform in regard of the current state of the environment.\n",
    "        It follows the terminology used in the Nature paper.\n",
    "\n",
    "        Attributes:\n",
    "            state (Tensor[input_shape]): The current environment state\n",
    "\n",
    "        Returns: Int >= 0 : Next action to do\n",
    "        \"\"\"\n",
    "        # Append the state to the short term memory (ie. History)\n",
    "        self._history.append(state)\n",
    "\n",
    "        # If policy requires agent to explore, sample random action\n",
    "        if self._explorer.is_exploring(self._num_actions_taken):\n",
    "            action = self._explorer(self.nb_actions)\n",
    "        else:\n",
    "            # Use the network to output the best action\n",
    "            env_with_history = self._history.value\n",
    "            q_values = self._action_value_net.eval(\n",
    "                # Append batch axis with only one sample to evaluate\n",
    "                env_with_history.reshape((1,) + env_with_history.shape)\n",
    "            )\n",
    "\n",
    "            self._episode_q_means.append(np.mean(q_values))\n",
    "            self._episode_q_stddev.append(np.std(q_values))\n",
    "\n",
    "            # Return the value maximizing the expected reward\n",
    "            action = q_values.argmax()\n",
    "\n",
    "        # Keep track of interval action counter\n",
    "        self._num_actions_taken += 1\n",
    "        return action\n",
    "\n",
    "    def observe(self, old_state, action, reward, done):\n",
    "        \"\"\" This allows the agent to observe the output of doing the action it selected through act() on the old_state\n",
    "\n",
    "        Attributes:\n",
    "            old_state (Tensor[input_shape]): Previous environment state\n",
    "            action (int): Action done by the agent\n",
    "            reward (float): Reward for doing this action in the old_state environment\n",
    "            done (bool): Indicate if the action has terminated the environment\n",
    "        \"\"\"\n",
    "        self._episode_rewards.append(reward)\n",
    "\n",
    "        # If done, reset short term memory (ie. History)\n",
    "        if done:\n",
    "            # Plot the metrics through Tensorboard and reset buffers\n",
    "            if self._metrics_writer is not None:\n",
    "                self._plot_metrics()\n",
    "            self._episode_rewards, self._episode_q_means, self._episode_q_stddev = [], [], []\n",
    "\n",
    "            # Reset the short term memory\n",
    "            self._history.reset()\n",
    "\n",
    "        # Append to long term memory\n",
    "        self._memory.append(old_state, action, reward, done)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" This allows the agent to train itself to better understand the environment dynamics.\n",
    "        The agent will compute the expected reward for the state(t+1)\n",
    "        and update the expected reward at step t according to this.\n",
    "\n",
    "        The target expectation is computed through the Target Network, which is a more stable version\n",
    "        of the Action Value Network for increasing training stability.\n",
    "\n",
    "        The Target Network is a frozen copy of the Action Value Network updated as regular intervals.\n",
    "        \"\"\"\n",
    "\n",
    "        agent_step = self._num_actions_taken\n",
    "\n",
    "        if agent_step >= self._train_after:\n",
    "            if (agent_step % self._train_interval) == 0:\n",
    "                pre_states, actions, post_states, rewards, terminals = self._memory.minibatch(self._minibatch_size)\n",
    "\n",
    "                self._trainer.train_minibatch(\n",
    "                    self._trainer.loss_function.argument_map(\n",
    "                        pre_states=pre_states,\n",
    "                        actions=Value.one_hot(actions.reshape(-1, 1).tolist(), self.nb_actions),\n",
    "                        post_states=post_states,\n",
    "                        rewards=rewards,\n",
    "                        terminals=terminals\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # Update the Target Network if needed\n",
    "                if (agent_step % self._target_update_interval) == 0:\n",
    "                    self._target_net = self._action_value_net.clone(CloneMethod.freeze)\n",
    "                    filename = \"models\\model%d\" % agent_step\n",
    "                    self._trainer.save_checkpoint(filename)\n",
    "\n",
    "    def _plot_metrics(self):\n",
    "        \"\"\"Plot current buffers accumulated values to visualize agent learning\n",
    "        \"\"\"\n",
    "        if len(self._episode_q_means) > 0:\n",
    "            mean_q = np.asscalar(np.mean(self._episode_q_means))\n",
    "            self._metrics_writer.write_value('Mean Q per ep.', mean_q, self._num_actions_taken)\n",
    "\n",
    "        if len(self._episode_q_stddev) > 0:\n",
    "            std_q = np.asscalar(np.mean(self._episode_q_stddev))\n",
    "            self._metrics_writer.write_value('Mean Std Q per ep.', std_q, self._num_actions_taken)\n",
    "\n",
    "        self._metrics_writer.write_value('Sum rewards per ep.', sum(self._episode_rewards), self._num_actions_taken)\n",
    "\n",
    "def transform_input(responses):\n",
    "    img1d = np.array(responses[0].image_data_float, dtype=np.float)\n",
    "    img1d = 255/np.maximum(np.ones(img1d.size), img1d)\n",
    "    img2d = np.reshape(img1d, (responses[0].height, responses[0].width))\n",
    "\n",
    "    from PIL import Image\n",
    "    image = Image.fromarray(img2d)\n",
    "    print(np.shape(image))\n",
    "    im_final = np.array(image.resize((84, 84)).convert('L')) \n",
    "\n",
    "    return im_final\n",
    "\n",
    "def interpret_action(action):\n",
    "    print(\"action index is\",action)\n",
    "    scaling_factor = 0.25\n",
    "    if action == 0:\n",
    "        quad_offset = (0, 0, 0)\n",
    "    elif action == 1:\n",
    "        quad_offset = (scaling_factor, 0, 0)\n",
    "    elif action == 2:\n",
    "        quad_offset = (0, scaling_factor, 0)\n",
    "    elif action == 3:\n",
    "#         quad_offset = (0, 0, scaling_factor)\n",
    "        quad_offset = (0, 0, 0)\n",
    "    elif action == 4:\n",
    "        quad_offset = (-scaling_factor, 0, 0)    \n",
    "    elif action == 5:\n",
    "        quad_offset = (0, -scaling_factor, 0)\n",
    "    elif action == 6:\n",
    "#         quad_offset = (0, 0, -scaling_factor)\n",
    "        quad_offset = (0, 0, 0)\n",
    "    \n",
    "    return quad_offset\n",
    "\n",
    "def compute_reward(quad_state, quad_vel, collision_info):\n",
    "    thresh_dist = 50\n",
    "    beta = 1\n",
    "\n",
    "    z = -10\n",
    "    pts = [np.array([-.55265, -31.9786, -19.0225]), np.array([48.59735, -63.3286, -60.07256]), np.array([193.5974, -55.0786, -46.32256]), np.array([369.2474, 35.32137, -62.5725]), np.array([541.3474, 143.6714, -32.07256])]\n",
    "\n",
    "    quad_pt = np.array(list((quad_state.x_val, quad_state.y_val, quad_state.z_val)))\n",
    "\n",
    "    if collision_info.has_collided:\n",
    "        reward = -100\n",
    "    else:    \n",
    "        dist = 10000000\n",
    "        for i in range(0, len(pts)-1):\n",
    "            dist = min(dist, np.linalg.norm(np.cross((quad_pt - pts[i]), (quad_pt - pts[i+1])))/np.linalg.norm(pts[i]-pts[i+1]))\n",
    "\n",
    "        #print(dist)\n",
    "        if dist > thresh_dist:\n",
    "            reward = -10\n",
    "        else:\n",
    "            reward_dist = (math.exp(-beta*dist) - 0.5) \n",
    "            reward_speed = (np.linalg.norm([quad_vel.x_val, quad_vel.y_val, quad_vel.z_val]) - 0.5)\n",
    "            reward = reward_dist + reward_speed\n",
    "\n",
    "    return reward\n",
    "\n",
    "def isDone(reward):\n",
    "    done = 0\n",
    "    if  reward <= -10:\n",
    "        done = 1\n",
    "    return done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "initX = -.55265\n",
    "initY = -31.9786\n",
    "initZ = -19.0225\n",
    "\n",
    "# connect to the AirSim simulator \n",
    "client = airsim.MultirotorClient()\n",
    "client.confirmConnection()\n",
    "client.enableApiControl(True)\n",
    "client.armDisarm(True)\n",
    "\n",
    "client.takeoffAsync().join()\n",
    "client.moveToPositionAsync(initX, initY, initZ, 5).join()\n",
    "client.moveByVelocityAsync(1, -0.67, -0.8, 5).join()\n",
    "time.sleep(0.5)\n",
    "\n",
    "# Make RL agent\n",
    "NumBufferFrames = 4\n",
    "SizeRows = 84\n",
    "SizeCols = 84\n",
    "NumActions = 7\n",
    "agent = DeepQAgent((NumBufferFrames, SizeRows, SizeCols), NumActions, monitor=True)\n",
    "\n",
    "# Train\n",
    "epoch = 100\n",
    "current_step = 0\n",
    "max_steps = epoch * 250000\n",
    "\n",
    "#responses = client.simGetImages([airsim.ImageRequest(3, airsim.ImageType.DepthPerspective, True, False)])\n",
    "responses = getScreenDepthVis()\n",
    "image = Image.fromarray(responses)\n",
    "\n",
    "current_state = np.array(image.resize((SizeRows,SizeCols)))\n",
    "#print(responses)\n",
    "#current_state = transform_input(responses)\n",
    "#print(transform_input(responses))\n",
    "#plt.imshow(transform_input(responses))\n",
    "\n",
    "plt.imshow(responses)\n",
    "while True:\n",
    "    action = agent.act(current_state)\n",
    "    quad_offset = interpret_action(action)\n",
    "    quad_vel = client.getMultirotorState().kinematics_estimated.linear_velocity\n",
    "    #print(\"begin move\")\n",
    "    #client.moveByVelocityAsync(quad_vel.x_val+quad_offset[0], quad_vel.y_val+quad_offset[1], quad_vel.z_val+quad_offset[2], 2).join()\n",
    "    client.moveByVelocityAsync(3*(quad_vel.x_val+quad_offset[0]), 3*(quad_vel.y_val+quad_offset[1]), 0, 2).join()\n",
    "    #time.sleep(0.5)\n",
    "    #print(\"end move\")\n",
    " \n",
    "    quad_state = client.getMultirotorState().kinematics_estimated.position\n",
    "    quad_vel = client.getMultirotorState().kinematics_estimated.linear_velocity\n",
    "    collision_info = client.simGetCollisionInfo()\n",
    "    reward = compute_reward(quad_state, quad_vel, collision_info)\n",
    "    done = isDone(reward)\n",
    "    print('Action, Reward, Done:', action, reward, done)\n",
    "\n",
    "    agent.observe(current_state, action, reward, done)\n",
    "\n",
    "    agent.train()\n",
    "\n",
    "\n",
    "    if done:\n",
    "        client.goHomeAsync()\n",
    "        client.moveToPositionAsync(initX, initY, initZ, 5).join()\n",
    "        client.moveByVelocityAsync(1, -0.67, -0.8, 5).join()\n",
    "        time.sleep(0.5)\n",
    "        current_step +=1\n",
    "\n",
    "    #responses = client.simGetImages([airsim.ImageRequest(3, airsim.ImageType.DepthPerspective, True, False)])\n",
    "    #current_state = transform_input(responses)\n",
    "\n",
    "    responses = getScreenDepthVis()\n",
    "    image = Image.fromarray(responses)\n",
    "\n",
    "    current_state = np.array(image.resize((SizeRows,SizeCols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(transform_input(responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(current_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=transform_input(client.simGetImages([airsim.ImageRequest(3, airsim.ImageType.DepthPerspective, True, False)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses.resize((100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.fromarray(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(image.resize((50,50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = getScreenDepthVis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
